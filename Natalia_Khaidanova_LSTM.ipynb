{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ad436b-3aa8-40d5-8354-c84ce23bb240",
   "metadata": {},
   "source": [
    "# LSTM for named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9ff86e-6610-4553-aebc-54af844abac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input, optimizers\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a9de52-cb14-456e-8a70-e3acef29c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt_path = '../../conll2003.train.conll'\n",
    "dev_txt_path = '../../conll2003.dev.conll'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b0791ee-9511-42c1-8f94-e4bd7d315680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86bb291e-56ee-4440-aef5-90ef1eb89827",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [train_txt_path, dev_txt_path]\n",
    "eval_split = 'dev'\n",
    "output_path = 'lsmt-out.csv'\n",
    "path_emb = '../../data/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "999bb10f-9cd4-4db4-bd7d-8c248367f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(paths):\n",
    "\n",
    "    data = []\n",
    "    sent_id = 1\n",
    "    for path in paths:\n",
    "        split = path.split('.')[-2]\n",
    "        with open(path) as infile:\n",
    "            lines = infile.read().split('\\n')\n",
    "        for n, line in enumerate(lines):\n",
    "            ll = line.split('\\t')\n",
    "            if len(ll) > 2:\n",
    "                d = dict()\n",
    "                d['Sentence #'] = f'Sentence: {sent_id}'\n",
    "                d['Word'] = ll[0]\n",
    "                d['POS'] = ll[1]\n",
    "                d['Chunk'] = ll[2]\n",
    "                d['Tag'] = ll[-1]\n",
    "                d['Split'] = split\n",
    "                data.append(d)\n",
    "\n",
    "            else:\n",
    "                sent_id += 1\n",
    "    data = pd.DataFrame(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2a53595-e1b0-4403-a531-70f7217dc3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Chunk</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>EU</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>rejects</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>B-VP</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>German</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>call</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>B-VP</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #     Word  POS Chunk     Tag  Split\n",
       "0  Sentence: 1       EU  NNP  B-NP   B-ORG  train\n",
       "1  Sentence: 1  rejects  VBZ  B-VP       O  train\n",
       "2  Sentence: 1   German   JJ  B-NP  B-MISC  train\n",
       "3  Sentence: 1     call   NN  I-NP       O  train\n",
       "4  Sentence: 1       to   TO  B-VP       O  train"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = convert_data(paths)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9310afd-c225-467c-a33a-b4c7ce782991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_map(data, token_or_tag, embedding_model=None):\n",
    "    \n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}   \n",
    "    \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51e87286-bb6e-4ff0-bd77-a98c9c48d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26883\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')\n",
    "print(len(token2idx))\n",
    "print(len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e177dd95-2f5b-4cda-b651-d12545c9d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format(path_emb, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1191dd40-e7c0-4d37-952e-7bf126f6c907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26884, 300)\n"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix with zero vectors for oov words\n",
    "emb_dim = 300\n",
    "embedding_matrix = np.zeros((len(token2idx) + 1, emb_dim))\n",
    "print(embedding_matrix.shape)\n",
    "for word, i in token2idx.items():\n",
    "    if word in w2v_model.key_to_index:\n",
    "        embedding_vector = w2v_model[word]\n",
    "    else:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37029404-3e6f-4dac-9ad6-b60da46716da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26884, 300)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions, store number of vector dimensions in variable\n",
    "print(embedding_matrix.shape)\n",
    "emb_dim = embedding_matrix.shape[1]\n",
    "print(emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "462952ee-e96b-4978-b28e-100286173445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Chunk</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Split</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>EU</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>train</td>\n",
       "      <td>11872</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>rejects</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>B-VP</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>7481</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>German</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>train</td>\n",
       "      <td>10272</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>call</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>521</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>B-VP</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>7614</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #     Word  POS Chunk     Tag  Split  Word_idx  Tag_idx\n",
       "0  Sentence: 1       EU  NNP  B-NP   B-ORG  train     11872        5\n",
       "1  Sentence: 1  rejects  VBZ  B-VP       O  train      7481        4\n",
       "2  Sentence: 1   German   JJ  B-NP  B-MISC  train     10272        1\n",
       "3  Sentence: 1     call   NN  I-NP       O  train       521        4\n",
       "4  Sentence: 1       to   TO  B-VP       O  train      7614        4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add index info to dataframe\n",
    "data['Word_idx'] = data['Word'].map(token2idx)\n",
    "data['Tag_idx'] = data['Tag'].map(tag2idx)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "329315e0-9019-4abf-abe8-44af05f5d91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_7640\\4279466741.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  data_group = data_fillna.groupby(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Chunk</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "      <th>Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
       "      <td>[NNP, VBZ, JJ, NN, TO, VB, JJ, NN, .]</td>\n",
       "      <td>[B-NP, B-VP, B-NP, I-NP, B-VP, I-VP, B-NP, I-N...</td>\n",
       "      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n",
       "      <td>[11872, 7481, 10272, 521, 7614, 16881, 9411, 8...</td>\n",
       "      <td>[5, 4, 1, 4, 4, 4, 1, 4, 4]</td>\n",
       "      <td>[train, train, train, train, train, train, tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[But, Fischler, agreed, to, review, his, propo...</td>\n",
       "      <td>[CC, NNP, VBD, TO, VB, PRP$, NN, IN, DT, NNP, ...</td>\n",
       "      <td>[O, B-NP, B-VP, I-VP, I-VP, B-NP, I-NP, B-PP, ...</td>\n",
       "      <td>[O, B-PER, O, O, O, O, O, O, O, B-ORG, O, O, O...</td>\n",
       "      <td>[24100, 26689, 20772, 7614, 11873, 5391, 21838...</td>\n",
       "      <td>[4, 3, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[train, train, train, train, train, train, tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>[The, Syrians, are, confused, ,, they, are, de...</td>\n",
       "      <td>[DT, NNPS, VBP, VBN, ,, PRP, VBP, RB, JJ, ,, C...</td>\n",
       "      <td>[B-NP, I-NP, B-VP, B-ADJP, O, B-NP, B-VP, I-VP...</td>\n",
       "      <td>[O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[8089, 854, 13577, 14901, 15792, 23441, 13577,...</td>\n",
       "      <td>[4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[train, train, train, train, train, train, tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>[The, youth, side, replied, with, 246, for, se...</td>\n",
       "      <td>[DT, NN, NN, VBD, IN, CD, IN, CD, .]</td>\n",
       "      <td>[B-NP, I-NP, I-NP, B-VP, B-PP, B-NP, B-PP, B-N...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[8089, 24700, 19792, 19891, 616, 8125, 8356, 1...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "      <td>[train, train, train, train, train, train, tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>[Men, 's, 3,000, metres, :]</td>\n",
       "      <td>[NN, POS, CD, NNS, :]</td>\n",
       "      <td>[B-NP, B-NP, I-NP, I-NP, O]</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "      <td>[24834, 7500, 12506, 8144, 16997]</td>\n",
       "      <td>[4, 4, 4, 4, 4]</td>\n",
       "      <td>[train, train, train, train, train]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence #                                               Word  \\\n",
       "0      Sentence: 1  [EU, rejects, German, call, to, boycott, Briti...   \n",
       "1     Sentence: 10  [But, Fischler, agreed, to, review, his, propo...   \n",
       "2    Sentence: 100  [The, Syrians, are, confused, ,, they, are, de...   \n",
       "3   Sentence: 1000  [The, youth, side, replied, with, 246, for, se...   \n",
       "4  Sentence: 10000                        [Men, 's, 3,000, metres, :]   \n",
       "\n",
       "                                                 POS  \\\n",
       "0              [NNP, VBZ, JJ, NN, TO, VB, JJ, NN, .]   \n",
       "1  [CC, NNP, VBD, TO, VB, PRP$, NN, IN, DT, NNP, ...   \n",
       "2  [DT, NNPS, VBP, VBN, ,, PRP, VBP, RB, JJ, ,, C...   \n",
       "3               [DT, NN, NN, VBD, IN, CD, IN, CD, .]   \n",
       "4                              [NN, POS, CD, NNS, :]   \n",
       "\n",
       "                                               Chunk  \\\n",
       "0  [B-NP, B-VP, B-NP, I-NP, B-VP, I-VP, B-NP, I-N...   \n",
       "1  [O, B-NP, B-VP, I-VP, I-VP, B-NP, I-NP, B-PP, ...   \n",
       "2  [B-NP, I-NP, B-VP, B-ADJP, O, B-NP, B-VP, I-VP...   \n",
       "3  [B-NP, I-NP, I-NP, B-VP, B-PP, B-NP, B-PP, B-N...   \n",
       "4                        [B-NP, B-NP, I-NP, I-NP, O]   \n",
       "\n",
       "                                                 Tag  \\\n",
       "0          [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]   \n",
       "1  [O, B-PER, O, O, O, O, O, O, O, B-ORG, O, O, O...   \n",
       "2  [O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O...   \n",
       "3                        [O, O, O, O, O, O, O, O, O]   \n",
       "4                                    [O, O, O, O, O]   \n",
       "\n",
       "                                            Word_idx  \\\n",
       "0  [11872, 7481, 10272, 521, 7614, 16881, 9411, 8...   \n",
       "1  [24100, 26689, 20772, 7614, 11873, 5391, 21838...   \n",
       "2  [8089, 854, 13577, 14901, 15792, 23441, 13577,...   \n",
       "3  [8089, 24700, 19792, 19891, 616, 8125, 8356, 1...   \n",
       "4                  [24834, 7500, 12506, 8144, 16997]   \n",
       "\n",
       "                                             Tag_idx  \\\n",
       "0                        [5, 4, 1, 4, 4, 4, 1, 4, 4]   \n",
       "1  [4, 3, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, ...   \n",
       "2  [4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "3                        [4, 4, 4, 4, 4, 4, 4, 4, 4]   \n",
       "4                                    [4, 4, 4, 4, 4]   \n",
       "\n",
       "                                               Split  \n",
       "0  [train, train, train, train, train, train, tra...  \n",
       "1  [train, train, train, train, train, train, tra...  \n",
       "2  [train, train, train, train, train, train, tra...  \n",
       "3  [train, train, train, train, train, train, tra...  \n",
       "4                [train, train, train, train, train]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group data by sentences\n",
    "# Fill na\n",
    "data_fillna = data.fillna(method='ffill', axis=0)\n",
    "# Groupby and collect columns\n",
    "data_group = data_fillna.groupby(\n",
    "['Sentence #'],as_index=False\n",
    ")['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx', 'Split'].agg(lambda x: list(x))\n",
    "# Visualise data\n",
    "data_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce33c343-86a9-425b-ba70-926539770881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_train_test_val(data_group, data, eval_split='dev'):\n",
    "    \n",
    "    # Get max token and tag length\n",
    "    n_token = len(list(set(data['Word'].to_list())))\n",
    "    n_tag = len(list(set(data['Tag'].to_list())))\n",
    "    print(n_token)\n",
    "    \n",
    "    # Pad tokens (X var) \n",
    "    tokens = data_group['Word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int64', padding='post', value= 26883)\n",
    "    print('padding', len(pad_tokens[0]))\n",
    "    \n",
    "    # Pad Tags (y var) and convert it into one hot encoding\n",
    "    tags = data_group['Tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int64', padding='post', value= tag2idx[\"O\"])\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    train_tokens = []\n",
    "    dev_tokens = []\n",
    "    train_tags = []\n",
    "    dev_tags = []\n",
    "    for i, row in data_group.iterrows():\n",
    "        if 'train' in row['Split']:\n",
    "            train_tokens.append(pad_tokens[i])\n",
    "            train_tags.append(pad_tags[i])\n",
    "        elif eval_split in row['Split']:\n",
    "            dev_tokens.append(pad_tokens[i])\n",
    "            dev_tags.append(pad_tags[i])\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntrain_tokens length:', len(train_tokens),\n",
    "        '\\nval_tokens:', len(dev_tokens),\n",
    "        '\\nval_tags:', len(dev_tags))\n",
    " \n",
    "    return np.array(train_tokens), np.array(dev_tokens),  np.array(train_tags), np.array(dev_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9159dd6-8ba8-4f17-8091-02a7ed6d71c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26883\n",
      "padding 113\n",
      "train_tokens length: 14041 \n",
      "train_tokens length: 14041 \n",
      "val_tokens: 3250 \n",
      "val_tags: 3250\n"
     ]
    }
   ],
   "source": [
    "train_tokens, dev_tokens, train_tags, dev_tags = get_pad_train_test_val(data_group, data, eval_split= eval_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad9a5562-375c-46a8-8373-e298ca222b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:  26884 \n",
      "output_dim:  300 \n",
      "input_length:  113 \n",
      "n_tags:  9\n",
      "emb dim 300\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(list(set(data['Word'].to_list()))) +1\n",
    "output_dim = emb_dim # number of dimensions\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)\n",
    "print('input_dim: ', \n",
    "      input_dim, '\\noutput_dim: ', \n",
    "      output_dim, '\\ninput_length: ', \n",
    "      input_length, '\\nn_tags: ', n_tags)\n",
    "print('emb dim', emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e5c51c8-8a38-4f8b-a27b-403a112a9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model(embedding_matrix, embedding_dim):\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Add Embedding layer original, trainable\n",
    "    # model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "    print(len(token2idx))\n",
    "    embedding_layer = Embedding(len(token2idx)+1 ,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=input_length,\n",
    "                            trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"sigmoid\")))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c99f4bad-2652-404d-98d2-b28333dfeca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for i in range(3):\n",
    "        hist = model.fit(X, y, batch_size=200, verbose=1, epochs=1, validation_split=0.2)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9094b2b-8dc3-4e78-841a-62a4f0710c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26883\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 113, 300)          8065200   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 113, 600)         1442400   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 113, 9)           5409      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,513,009\n",
      "Trainable params: 1,447,809\n",
      "Non-trainable params: 8,065,200\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "57/57 [==============================] - 1350s 23s/step - loss: 0.1907 - accuracy: 0.9626 - val_loss: 0.0631 - val_accuracy: 0.9799\n",
      "57/57 [==============================] - 1451s 26s/step - loss: 0.0540 - accuracy: 0.9826 - val_loss: 0.0357 - val_accuracy: 0.9911\n",
      "57/57 [==============================] - 1461s 26s/step - loss: 0.0337 - accuracy: 0.9916 - val_loss: 0.0300 - val_accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "embedding_dim = 300\n",
    "model_bilstm_lstm = get_bilstm_lstm_model(embedding_matrix, embedding_dim)\n",
    "plot_model(model_bilstm_lstm)\n",
    "results['with_add_lstm'] = train_model(train_tokens, train_tags, model_bilstm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8e9e751-b32c-4bb6-981b-dd5fcc8c40af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3250/3250 [==============================] - 4255s 1s/step - loss: 0.0304 - accuracy: 0.9920\n",
      "test loss, test acc: [0.03041241690516472, 0.992005467414856]\n"
     ]
    }
   ],
   "source": [
    "results = model_bilstm_lstm.evaluate(dev_tokens, np.array(dev_tags), batch_size=1)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c4fb6cf-dac7-4f5a-958b-a37f27ddbfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "751efb46-6369-46e5-8f9e-03997d85cca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 352s 3s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_bilstm_lstm.predict(dev_tokens)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_dev = np.argmax(dev_tags, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7cea52bf-7f96-4f9b-b091-20e59285ddc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_labels = []\n",
    "for tag in y_pred:\n",
    "    for i in tag:\n",
    "        label = idx2tag[i]\n",
    "        pred_labels.append(label)\n",
    "    \n",
    "dev_labels = []\n",
    "for tag in y_dev:\n",
    "    for i in tag:\n",
    "        label = idx2tag[i]\n",
    "        dev_labels.append(label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27da6aa7-59d1-4648-aa8c-f851374f86c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367250\n",
      "367250\n"
     ]
    }
   ],
   "source": [
    "print(len(pred_labels))\n",
    "print(len(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88df6096-a71a-47e4-9a2a-11796d32140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(dev_labels)\n",
    "classes = classes.tolist()\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb7e2a09-00cd-4369-a37c-a518c03d2317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = classes.copy()\n",
    "classes.pop()\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ed72f90-d22e-4861-953f-7802fff2fe8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.916     0.643     0.755      1837\n",
      "      B-MISC      0.788     0.696     0.739       922\n",
      "       B-ORG      0.480     0.879     0.621      1341\n",
      "       B-PER      0.923     0.832     0.875      1842\n",
      "       I-LOC      0.758     0.183     0.295       257\n",
      "      I-MISC      0.715     0.341     0.462       346\n",
      "       I-ORG      0.607     0.483     0.538       751\n",
      "       I-PER      0.957     0.747     0.839      1307\n",
      "\n",
      "   micro avg      0.749     0.702     0.725      8603\n",
      "   macro avg      0.768     0.601     0.641      8603\n",
      "weighted avg      0.802     0.702     0.727      8603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev_labels, pred_labels, labels=classes, digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5475d432-c700-4803-b941-f97ecb27f299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER']\n",
      "[[1181   84  431   13    5    0    9    0]\n",
      " [  11  642  130   13    0    3    5    0]\n",
      " [   8   22 1179   13    0    0   20    0]\n",
      " [   3    3  147 1533    0    1    7   21]\n",
      " [  48    0   34    2   47    8   85    9]\n",
      " [   1   37   35    1    4  118   43    4]\n",
      " [  25    4  209    6    6    7  363    9]\n",
      " [   3    0   35   68    0    1   22  976]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(classes)\n",
    "print(sklearn.metrics.confusion_matrix(dev_labels, pred_labels, labels=classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
